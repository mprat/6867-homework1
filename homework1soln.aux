\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Exercise 1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Convex function with known minimum value and it's analytical derivative. The dots are the calculated minima according to gradient descent. (b) Contour plot of the gradient of a scalar function with vector-valued arguments. The dots are the calculated minima according to gradient descent.\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1-2-ab}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The solid line is the function, the dashed line is its analytical derivative, and the series of dots is the series of minimum guesses from gradient descent. (a) Initial guess: 5, step: 0.5, threshold: 0.1 converges in 9 iterations. (b) Initial guess: -2, step: 0.2, threshold: 0.1 converges in 20 iterations. (c) Initial guess: 4, step: 2, threshold: 0.1 converges in 59992 iterations. (d) Initial guess: 0, step: 0.5, threshold: 0.1 converges in 1 iteration, but does not converge to a minimum. \relax }}{2}}
\newlabel{fig:1-2-c}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The analytical and numerical gradients for the functions (a) upside-down Gaussian, (b) $f(x, y)= (x + 2)^2 + y^2$, and (c) $f(x) = \mathop {\mathgroup \symoperators sin}\nolimits (x + {\begingroup \pi \endgroup \over 2})$. The solid red line is the analytical gradient and the dashed blue line is the numerical gradient.\relax }}{2}}
\newlabel{fig:1-3-a}{{3}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Steps to convergence given a similar set of parameters. GD is gradient descent, \textsc  {BFGS} is from SciPy. For gradient descent, all step sizes were $0.5$ and thresholds were $0.01$.\relax }}{2}}
\newlabel{tbl:1-4-a}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Exercise 2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plots replicating Figure 1.4 in Bishop for different values of $M$ using different methods. The blue is generated with the analytical solution of the SSE, the green with the gradient descent method on the SSE, and the red using Python's \textsc  {BFGS} optimizer.\relax }}{3}}
\newlabel{fig:2-1}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Table 1.1 from Bishop replicated using (a) analytical solutions, (b) gradient descent, and (c) the Scipy \textsc  {BFGS} optimizer\relax }}{3}}
\newlabel{tbl:bishop-1-1}{{5}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Values of the SSE and the analytical and numerical gradients for the given values of $M$ calculated using the $w$ coefficients calculated from the analytical solution.\relax }}{3}}
\newlabel{tbl:sse}{{2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Exercise 3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Predicted regressions using ridge regression for various values of $M$ and $\lambda $\relax }}{4}}
\newlabel{fig:3-1}{{6}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The optimal regression lines for (a) Set A and (b) Set B and their corresponding $M$ and $\lambda $ values. (c) is a plot of all the data together (sets A and B as well as the training data)\relax }}{5}}
\newlabel{fig:3-2}{{7}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A plot of the MSE (mean square error) as a function of different values of $\lambda $\relax }}{5}}
\newlabel{fig:3-3}{{8}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Exercise 4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A plot of the LAD (least absolute deviation) as a function of different values of $\lambda $ for $M = 1$. The x-values are on a log scale.\relax }}{6}}
\newlabel{fig:4-1}{{9}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A plot of the Lasso as a function of different values of $\lambda $ for $M = 1$. The x-values are on a log scale.\relax }}{7}}
\newlabel{fig:4-2}{{10}{7}}
