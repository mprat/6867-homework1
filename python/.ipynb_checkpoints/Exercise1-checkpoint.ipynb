{
 "metadata": {
  "name": "",
  "signature": "sha256:1482563e14dbb53f982545af0787db0cdcf616c31f17c298a65bdc712ad013cc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise 1: Implement a basic gradient descent procedure to minimize scalar functions of a vector argument. Write it generically, so that you can easily specify the objective function and the function to compute the gradient. You should be able to specify the initial guess, the step size and the convergence criterion (a threshold such that the algorithm terminates when the difference in objective function value on two successive steps is below this threshold). Test your gradient descent procedure on some functions whose optimal value you know, e.g. a quadratic bowl or the (negative of) a Gaussian probability density function. Make sure that you try functions of more than one variable and that you try at least one convex function and one very non-convex function with multiple minima. Discuss the effect of the choice of starting guess, the step size, and the convergence criterion on the resulting solution. Write code to approximate the gradient of a function numerically at a given point using cen- tral differences (see the \u201cFinite difference\u201d article in Wikipedia). Verify its behavior on the functions you used in the question above by comparing the analytic and numerical gradients at various points. Compare the behavior of your gradient descent procedure with one the more sophisticated optimizers available in Matlab (e.g. fminunc) or scipy.optimize (e.g. fmin_bfgs). A good metric for comparison is the number of function evaluations required to reach conver- gence. You should make sure your code is able to keep track of function calls and call the Matlab or Scipy optimizers so that they print this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as scipy\n",
      "from scipy import optimize\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from mpl_toolkits.mplot3d import proj3d\n",
      "matplotlib.rcParams.update({'font.size': 14, 'font.family': 'serif', 'text.usetex': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gradient_descent(f, f_prime, initial_guess, step_size, thresh):\n",
      "    previous = initial_guess\n",
      "    next_guess = step(initial_guess, step_size, f_prime)\n",
      "    num_iterations = 1\n",
      "    \n",
      "    if isinstance(initial_guess, int):\n",
      "        while(abs(next_guess - previous) >= thresh):\n",
      "            previous = next_guess\n",
      "            next_guess = step(next_guess, step_size, f_prime)\n",
      "            num_iterations += 1\n",
      "    else:\n",
      "        while(numpy.linalg.norm(next_guess - previous) >= thresh):\n",
      "            previous = next_guess\n",
      "            next_guess = step(next_guess, step_size, f_prime)\n",
      "            num_iterations += 1\n",
      "    \n",
      "    print \"Num iterations = \", num_iterations\n",
      "    return np.append(next_guess, f(next_guess))\n",
      "\n",
      "def gradient_descent_return_all_guesses(f, f_prime, initial_guess, step_size, thresh):\n",
      "    all_guesses = []\n",
      "    all_guesses.append(np.array([initial_guess, f(initial_guess)]))\n",
      "    previous = initial_guess\n",
      "    next_guess = step(initial_guess, step_size, f_prime)\n",
      "    num_iterations = 1\n",
      "    \n",
      "    while(abs(next_guess - previous) >= thresh):\n",
      "        previous = next_guess\n",
      "        next_guess = step(next_guess, step_size, f_prime)\n",
      "        all_guesses.append(np.array([next_guess, f(next_guess)]))\n",
      "        num_iterations += 1\n",
      "    \n",
      "    print \"Num iterations = \", num_iterations\n",
      "    return all_guesses\n",
      "    \n",
      "def step(x_n, step_size, f_prime):\n",
      "#     print \"x_n = \", x_n\n",
      "#     print \"f prime of x_n = \", f_prime(x_n)\n",
      "#     print \"f prime of x_n = \", x_n - step_size* np.array(f_prime(x_n)).flatten().reshape((2, -1))\n",
      "#     print \"size xn = \", x_n.shape\n",
      "#     print x_n\n",
      "#     print type(x_n)\n",
      "    if isinstance(x_n, int) or isinstance(x_n, float):\n",
      "        return x_n - step_size*f_prime(x_n)\n",
      "    else:\n",
      "        return x_n - step_size*np.array(f_prime(x_n)).flatten().reshape((x_n.shape[0], -1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gradient descent is an iterative procedure to find the local minimum of a function $f(x)$ by successively looking at values of the function proportional to the direction of the gradient of the function. The iteration stops when the result from two successive steps yields a difference below a specified threshold. This means that once the local minima is reached, the moving along the gradient of a given function will not yield significantly different results. This procedure can be written mathematically as: \n",
      "$$x_{n + 1} = x_n - \\eta*\\nabla f(x_n).$$\n",
      "\n",
      "A start value $x_0$ and a step size $\\eta$ is given to start the procedure, as well as a threshold $\\beta$. When $|x_{n + 1} - x_n| < \\beta$, the procedure stops.\n",
      "\n",
      "Example: function of one variable, known minima, function of one variable. Minimum happens at: \n",
      "\n",
      "$$x = -\\frac{b}{2a} = -\\frac{4}{2} = -2$$ and \n",
      "$$y = f(-2) = 1 - 2 = 0 $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}