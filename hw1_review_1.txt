REVIEW 1:
Average = (4 + 4 + 4 + 5) / 4 = 4.25
Problem 1:
Score: 4
The author does a good job explaining the pseudocode for gradient descent. The notation x' and y' is confusing, because in some mathematical arenas this means derivative. However, from the context it is clear that the author means this to be the next calculated value of the gradient descent. The author also does a good job of both presenting the salient results from the tests of known functions on gradient descent through tables and then summarizing the salient points in written form. For the case of a non-convex function, the author does not consider starting at a local maximum point to see how the gradient descent function behaves. The author summarizes the effectiveness of the finite difference and the analytical gradient well using the SSE, but it might have been nice to also include a graph showing plots of the two functions (analytical and numeric). The summary of the comparison with MATLAB / own implementations was a bit lacking - it would have been nice to explain how the functions behave different on non-convex inputs, or give more than one example to illustrate their comparison. 

Problem 2:
Score: 4
The author does a good job analytically explaining how the SSE and SSE gradient work. For the calculated values of Beta presented, it would have been nice to show actual graphs instead of just tables, since it is hard to evaluate correctness of fit based on a table when the SSE numerically is not provided for the particular M values. Additionally, while the values presented for the Beta coefficients are correct, there is minimal discussion about the interesting peculiarities around choosing input values. While the author provides an explanation for why the values of M are not great, it is not explained well or easily understood. 

Problem 3:
Score: 4
While the explanation for ridge regression is correct, it overlooks the fact that the beta_0 (constant) coefficient is not calculated in the analytical formulation given. The author's notation for M=1 up to M=10 is inconsistent with Bishop - M=1 as the author noted (in Bishop it is M=0) should mean a constant function, but the figures displayed show a line with some slope, so there is a mistake in the author's code. However, the array of figures is a fantastic way of illustrating the author's point, since it clearly shows variations on M and lambda in different scenarios. The author does a good job of explaining the method used for determining the values of M and lambda, however, the explanation is written using grammatical mistakes and convoluted or incorrect wording, making it hard to follow. The approach is unique in that the author decided to run an optimization algorithm on first W and then lambda rather than manually choosing values of lambda that decrease the validation error. For the ridge regression on training / validation / test data, while the data is clearly presented in a value-table format, it is hard to intuitively understand. A graphical representation of the coefficients produced would be more simple to understand. The explanation offered on training set B outliers and data set size are great observations about why the model does not perform exactly as desired. For the blogset data given in the problem set, the author sets out an easy-to-follow procedure, but the data presented as results do not seem to match expected results, as the lambda value is very high, and the error seems to not have been normalized according to the number of data points in the same (in both 3.2 and 3.3). It is also unclear whether Figure 3 belongs with 3.2 or 3.3, as it is not explained in the text of the writeup. The author uses the technique of normalizing the data to run the regression and clearly explains that in the writeup, making the method very clear to understand.

Problem 4:
Score: 5
The author's explanation of the methods used for both LAD and Lasso are very easy to follow and explain clearly their approach to optimization, making it clear to understand the difference in approach between this question and question 3. The included and referenced plots are fantastic at visualizing what the author's conclusions were, and the written explanations explained the plots concisely. However, it would have been nice to include the number of the error as calculated for each validation / training set, since there was no testing set used to evaluate model effectiveness. The author's discussion of observing nearly-zero coefficients using Lasso and a brief comparison to more sophisticated optimizers was well written and very informative about the author's understanding of the problem. However, I will disagree with the author's claim that the Lasso is more robust to outliers, given that the Lasso result still looks similar to the LAD result on training set B. The author's discussion about LAD / Lasso / loss / relation to error distributions is well-written and informative. 
---------
---------
---------
---------
REVIEW 2:
Overall score: (5 + 5 + 5 + 5) / 4 = 5

Problem 1:
Score: 5
The author does a good job of using a graph to show how the numerical gradient approximates the analytic gradient. Illustrating the gradient descent for various values of delta was a good choice by the author to show that the classical gradient descent method works. The author also gives an interesting better method for calculating gradient descent using line search and gives examples of convergence according to different parameters, giving good summaries of the results in textual form. However, the author does not discuss the case of non-convex functions versus convex functions using gradient descent. In the author's analysis, it is hard to tell whether the values / experiments in this section were made with the line search or the classical gradient descent methods. The discussion and comparison to BFGS was well-summarized and clear.

Problem 2:
Score: 5
The author provides a good discussion of regression and SSE, and shows / references good figures for the various model classes. One suggestion is to have the plots all be on the same scale, so it is visually easier to see the wide swings of the M=9 model. Showing the gradient of the SSE compared analytically and numerically also gives the reader a good idea that the numerical SSE estimate is correct. Showing the data generating function alongside the experiment graphs was a good choice by the author. The author does a good job of summarizing numeric and analytic solutions for all of the model classes, noting interesting plots where step size values produce interesting results. Comparison to the BFGS algorithm are well-explained, and the results are consistent and expected. The author does a good job of using plots to illustrate their points. The author's explanation of the 9-th order Taylor series expansion for M=9 regression convergence is a good one.

Problem 3:
Score: 5
The author does a good job of explaining their method setup with sets A, B, and validate, and does an interesting comparison with switching the training / validation datasets. The technical discussion / introduction of ridge regression is also informative, but the author did not discuss the special case of calculating w_0. The discussion about outliers is informative, and supported by the provided table. However, from the author's discussion, it is assumed that only 2 values of lambda are tested in the validation step (before testing on the validation dataset), therefore not proving that the value of lambda chosen was optimal. It seems from the lambda trend given in the table that the error will converge even more for smaller values of lambda. The discussion of the method and results on the blog dataset are very detailed, and include the discussion about the optimality of lambda. However, I will disagree with the author that in fact the value of M is 1, not 280, and the coefficients being optimized are the polynomial weights of the individual features - the entire feature vector is treated as single vector input rather than 280 inputs. I am not sure how in practice these two approaches are different other than the linear algebra.

Problem 4:
Score: 5
The author does a good job of describing the differences between the LAD / Lasso / OLS / ridge approaches and does a good job of showing plots / tables that summarize the salient findings. However, the same problem with the choice of lambda is found in this discussion - there is no guarantee or discussion that the lambdas chosen are the optimal lambdas. The author also presents a good discussion of outliers / noisy data and the benefits of using one regularizer over another. 