\documentclass[10pt]{article}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumerate}

\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

\begin{document}

\section{Exercise 1}

% Gradient descent is an iterative procedure to find the local minimum of a function $f(x)$ by successively looking at values of the function proportional to the direction of the gradient of the function. The iteration stops when the result from two successive steps yields a difference below a specified threshold. This means that once the local minimum is reached, movement along the gradient of a given function will not yield significantly different results. For a scalar function with vector argument inputs, this procedure can be written mathematically as: 
% $$x_{n + 1} = x_n - \eta*\nabla f(x_n).$$

% A start value $x_0$ and a step size $\eta$ is given to start the procedure, as well as a threshold $\beta$. When $|x_{n + 1} - x_n| < \beta$, the procedure stops. It is possible to define different step sizes $\eta$ for each iteration of the gradient descent procedure, but this analysis assumes a constant step size. 

% Testing this gradient descent optimizer on an upside-down Gaussian function with mean $6$ and variance $4$, we expect the minimum to occur at $(6, -1)$, since there is one global minimum. If the function and gradient are defined as: 
% $$f(x) = -e^{\frac{(x - 6)^2}{8}} \qquad \nabla f = \frac{x - 6}{4} e^{\frac{(x - 6)^2}{8}}$$
% (see Figure \ref{fig:gaus}), then the gradient descent algorithm always converges to the correct global minimum. For functions with one minimum, the larger the step value, the faster the algorithm will converge, regardless of how far the initial guess is from the minimum. The threshold dictates how close the numerical solution is from the analytical solution (see Figure \ref{tbl:gaus}).

% \begin{figure}[!ht]
% 	\centering
% 	\makebox[\linewidth][c]{%}
% 	\begin{subfigure}[h]{0.55\textwidth}
% 	\includegraphics[width=\textwidth]{exercise1a.png}
% 	\caption{Graph of $f(x)$ and its gradient}
% 	\label{fig:gaus}
% 	\end{subfigure}
% 	\begin{subtable}[h]{0.55\textwidth}
% 	\centering
% 	\vspace{12mm}
% 	\begin{tabular}[h]{cccc}
% 		Guess & Step & Iterations & Minimum \\\hline
% 		5 & 0.5 & 21 & (5.934, -.999) \\\hline
% 		5 & 1 & 13 & (5.974, -1.000)  \\\hline
% 		2 & 0.5 & 47 & (5.938, -1.000) \\\hline
% 		2 & 5 & 4 & (5.998, -1.000) \\\hline
% 	\end{tabular}
% 	\vspace{12mm}
% 	\caption{Parameters and results of gradient descent. The threshold was always $0.01$}
% 	\label{tbl:gaus}
% 	\end{subtable}
% 	}\\
% 	\caption{Results for $f(x) = $-Gaussian$(6, 4)$}
% \end{figure}

% The same algorithm can be applied to scalar functions with vector inputs. For example, 
% $$ f(x, y) = (x + 2)^2 + y^2 \qquad \nabla f = 2(x + 2) + 2y$$
% And has one global minimum. In choosing parameters, the smaller the step size and the smaller the threshold, the more iterations are required for the algorithm to converge.

% \begin{figure}[!ht]
% 	\centering
% 	\makebox[\linewidth][c]{%}min}
% 	\begin{subfigure}[b]{0.55\textwidth}
% 		\includegraphics[width=\textwidth]{exercise1c.png}
% 		\caption{$f(x, y)$, $\nabla f$, and the estimated minimum}
% 		\label{fig:nomin}
% 	\end{subfigure}
% 	\begin{subtable}[b]{0.55\textwidth}
% 	\centering
% 	\begin{tabular}[b]{ccccc}
% 		Guess & Step & Thresh & Iterations \\\hline
% 		(0, 0) & 0.5 & 0.01 & 2 \\\hline
% 		(5, 5) & 0.5 & 0.01 & 2  \\\hline
% 		(5, 5) & 0.1 & 0.01 & 25 \\\hline
% 		(5, 5) & 0.1 & 0.001 & 35 \\\hline
% 	\end{tabular}
% 	\vspace{10mm}
% 	\caption{Parameters and results of gradient descent. The minimum always came out to be $(-2, 0, 0)$}
% 	\label{tbl:nomin}
% 	\end{subtable}
% 	}\\
% 	\caption{Using gradient descent to estimate the minimum of a scalar function with vector inputs}
% \end{figure}

% For functions without a global minimum (non-convex functions), the choice of input parameters dictates not only how fast the algorithm converges, but also the value at which it converges. For example, the function
% $$f(x) = \sin(x + \frac{\pi}{2}) + 3 \qquad \nabla f = \cos(x + \frac{\pi}{2})$$
% has local minima at $(2(n + 1)\pi, 2)$ for all $n$ (see Figure \ref{fig:nomin}). When the initial guess for gradient descent is chosen to be closer to the local minima at $-\pi$, the algorithm estimates that to be the local minimum, and vice versa for $\pi$. If the initial guess is given at $0$, which is a local maximum, the algorithm does not descend and stops after one iteration.

% \begin{figure}[!ht]
% 	\centering
% 	\makebox[\linewidth][c]{%}min}
% 	\begin{subfigure}[b]{0.6\textwidth}
% 		\includegraphics[width=\textwidth]{exercise1d.png}
% 		\caption{A sinusoid and it's derivative, together with the estimated minima}
% 		\label{fig:2var}
% 	\end{subfigure}
% 	\begin{subtable}[b]{0.6\textwidth}
% 	\centering
% 	\begin{tabular}[b]{ccccc}
% 		Guess & Step & Thresh & Iterations & Minimum \\\hline
% 		5 & 0.5 & 0.01 & 9 & ($\pi$, 2) \\\hline
% 		-5 & 0.5 & 0.01 & 9 & ($-\pi$, 2)  \\\hline
% 		0 & 0.5 & 0.01 & 1 & (0, 4) \\\hline
% 		0.5 & 0.5 & 0.01 & 12 & ($\pi$, 2) \\\hline
% 		0.2 & 0.5 & 0.01 & 14 & ($\pi$, 2) \\\hline
% 		0.7 & 0.5 & 0.01 & 11 & ($\pi$, 2) \\\hline
% 	\end{tabular}
% 	\vspace{10mm}
% 	\caption{Parameters and results of gradient descent}
% 	\label{tbl:nomin}
% 	\end{subtable}
% 	}\\
% 	\caption{Using gradient descent on a function with many local minima}
% \end{figure}

% \begin{figure}[!ht]
% \centering
% 	\makebox[\linewidth][c]{%}min}
% \begin{subfigure}[ht]{0.4\textwidth}
% 		\includegraphics[width=\textwidth]{exercise1e.png}
% 		\label{fig:cdgaus}
% 		\caption{}
% 	\end{subfigure}
% 	\begin{subfigure}[ht]{0.4\textwidth}
% 		\includegraphics[width=\textwidth]{exercise1f.png}
% 		\label{fig:cdsin}
% 		\caption{}
% 	\end{subfigure}
% 	\begin{subtable}[ht]{0.4\textwidth}
% 	\centering
% 	\begin{tabular}[ht]{ccccc}
% 		(x, y) & Analytic $\nabla f$ & Numeric\\\hline
% 		(-2, -2) & (0, -4) & (0, -4) \\\hline
% 		(-2, -1) & (0, -2) & (0, -2) \\\hline
% 		(-2, 0) & (0, 0) & (0, 0) \\\hline
% 		(-2, 1) & (0, 2) & (0, 2) \\\hline
% 		(-2, 2) & (0, 4) & (0, 4) \\\hline
% 	\end{tabular}
% 	\vspace{5mm}
% 	\label{tbl:2varcd}
% 	\caption{}
% 	\end{subtable}
% 	}\\
% 	\label{fig:centraldiff}
% 	\caption{The step size for calculating the central difference was $1$. (a) The analytic and numeric approximations of Figure \ref{fig:gaus}. (b) The analytic and numeric approximations of Figure \ref{fig:nomin}.  (c) The analytic and numeric approximations of Figure \ref{fig:2var}.}
% \end{figure}

% The gradient of a function can either be specified analytically or numerically approximated with central differences using $\delta_{h}[f](x) = f(x + 0.5h) - f(x - 0.5h)$. From a numerical standpoint, using either the analytical or numerical approximation to the gradient is approximately equivalent, given the correct choice of step size (see Figure 4).


% Software packages like SciPy implement a different algorithm for estimating minima. Table 1 gives a comparison of the number of iterations of the algorithm needed to find a solution. In general, the \textsc{bfgs} optimizer in SciPy performs better than the simple gradient descent algorithm presented here. Both algorithms came up to the same solution within a few significant digits.

The implemented gradient descent procedure was tested on three functions: 
\begin{enumerate}[(a)]
	\item A convex function: $f(x) = -Gaussian(6, 4)$ (an upside-down Gaussian with mean $6$ and variance $4$), who's minimum value is at $(6, -1)$. 
	\item A scalar convex function of two variables: $f(x, y) = (x + 2)^2 + y^2$, who's minimum value is at $(-2, 0)$.
	\item A non-convex function: $f(x) = \sin(x + \frac{\pi}{2})$, who has multiple minima at $(n \pi, -1)$ for all values of $n$.
\end{enumerate}

Figure \ref{fig:1-2-ab} shows the result of calculating the gradient descent on functions (a) and (b) written above. The dots are the calculated minima from gradient descent calculated at multiple step sizes, initial guesses, and thresholds. For convex functions with one minimum, the threshold is the biggest indicator of how many steps the procedure takes to converge. The threshold also determines how close the result is to the real minimum - the smaller the threshold, the closer the result is to the real minimum. Because there is one global minimum, the initial guess does not affect the end result of the function or significantly change the number of iterations required to converge.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=.66\textwidth]{exercise1-2-ab.pdf}
	\caption{(a) Convex function with known minimum value and it's analytical derivative. The dots are the calculated minima according to gradient descent. (b) Contour plot of the gradient of a scalar function with vector-valued arguments. The dots are the calculated minima according to gradient descent.}
	\label{fig:1-2-ab}
\end{figure}

Figure \ref{fig:1-2-c} shows the result of using gradient descent on function (c) with different parameters. The sequence of dots in each case represents the guesses that gradient descent takes to be the minimum. In general, the choice of starting point affects which local minimum gradient descent will find. If the starting point chosen is a local maximum, the procedure will not step towards a minimum because the gradient is zero at the maximum. If the step size is large, the procedure will overshoot the minimum and zig-zag back to the desired minimum, taking a large number of iterations to converge. 

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{exercise1-2-c.pdf}
	\caption{The solid line is the function, the dashed line is its analytical derivative, and the series of dots is the series of minimum guesses from gradient descent. (a) Initial guess: 5, step: 0.5, threshold: 0.1 converges in 9 iterations. (b) Initial guess: -2, step: 0.2, threshold: 0.1 converges in 20 iterations. (c) Initial guess: 4, step: 2, threshold: 0.1 converges in 59992 iterations. (d) Initial guess: 0, step: 0.5, threshold: 0.1 converges in 1 iteration, but does not converge to a minimum. }
	\label{fig:1-2-c}
\end{figure}

Approximating a function's gradient using central differences is very accurate if using an $h$ of $1$. Figure \ref{fig:1-3-a} shows graphs of either the gradient function or gradient vector field for the three functions tested above. It is hard to tell the difference between the analytical and numerical functions, so using central differences to numerically estimate the gradient is effective.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{exercise1-3-a.pdf}
	\caption{The analytical and numerical gradients for the functions (a) upside-down Gaussian, (b) $f(x, y)= (x + 2)^2 + y^2$, and (c) $f(x) = \sin(x + \frac{\pi}{2})$. The solid red line is the analytical gradient and the dashed blue line is the numerical gradient.}
	\label{fig:1-3-a}
\end{figure}

The \textsc{BFGS} optimizer in SciPy uses a more sophisticated method for estimating minima, so in general, the number of function evaluations needed before convergence is smaller. Table \ref{tbl:1-4-a} shows a comparison of the gradient descent and \textsc{BFGS} optimizer, where in all cases the step size was $0.2$ and the threshold was $0.01$.

\begin{table}[!ht]
\centering
\makebox[\textwidth][c]{
\begin{tabular}[ht]{ccccc}
Function & Initial Guess & Algorithm & Evaluations & Min at\\\hline
\multirow{2}{*}{$f(x) = -$Gaussian$(6, 4)$} & \multirow{2}{*}{$x = 5$} & GD & 34 & $x = 5.81$ \\
 & & \textsc{BFGS} & 6 & $x = 6.00$ \\\hline
\multirow{2}{*}{$f(x, y) = (x + 2)^2 + y^2$} & \multirow{2}{*}{$(x, y) = (5, 5)$} & GD & 13 & $(x, y) = (-2, 0)$\\
 & & \textsc{BFGS} & 4 & $(x, y) = (-2, 0)$ \\\hline
\multirow{2}{*}{$f(x) = \sin(x + \frac{\pi}{2})$} & \multirow{2}{*}{$x = 5$} &  GD & 20 & $x = 3.15$ \\
 & &  \textsc{BFGS} & 6 & $x = 3.14$ \\
\end{tabular}
}
\caption{Steps to convergence given a similar set of parameters. GD is gradient descent, \textsc{BFGS} is from SciPy. For gradient descent, all step sizes were $0.5$ and thresholds were $0.01$.}
\label{tbl:1-4-a}
\end{table}

% \newpage
\section{Exercise 2}

\begin{figure}[!ht]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.25\textwidth]{exercise2-1.pdf}}
	\caption{Plots replicating Figure 1.4 in Bishop for different values of $M$ using different methods. The blue is generated with the analytical solution of the SSE, the green with the gradient descent method on the SSE, and the red using Python's \textsc{BFGS} optimizer.}
	\label{fig:2-1}
\end{figure}

\begin{figure}[!ht]
\centering
\makebox[\textwidth][c]{
\begin{subtable}[!ht]{0.5\textwidth}
\centering
\scalebox{0.8}{
\begin{tabular}{l|rrrr}
	& $M = 0$ & $M = 1$ & $M = 6$ & $M = 9$ \\\hline
	$\hat{w}_0$ & $0.186$ & $0.820$ & $0.353$ & $0.349$ \\
	$\hat{w}_1$ & & $-1.268$ & $2.619$ & $232.411$\\
	$\hat{w}_2$ & & & $32.096$ & $-5322.833$\\
	$\hat{w}_3$ & & & $-206.273$ & $48577.404$\\
	$\hat{w}_4$ & & & $399.001$ & $-231682.150$\\
	$\hat{w}_5$ & & & $-332.709$ & $640159.305$\\
	$\hat{w}_6$ & & & $105.163$ & $-1061992.54$\\
	$\hat{w}_7$ & & & & $1042586.68$ \\
	$\hat{w}_8$ & & & & $-557781.754$\\
	$\hat{w}_9$ & & & & $125223.393$\\
\end{tabular}
}
\end{subtable}
\hspace{3mm}
\begin{subtable}[!ht]{0.5\textwidth}
\centering
\scalebox{0.8}{
\begin{tabular}{l|rrrr}
	& $M = 0$ & $M = 1$ & $M = 6$ & $M = 9$ \\\hline
	$\hat{w}_0$ & $0.003$ & $0.499$ & $0.623$ & $-0.014$ \\
	$\hat{w}_1$ & & $-0.502$ & $5.577$ & $6.018$ \\
	$\hat{w}_2$ & & & $-14.465$ & $-12.820$ \\
	$\hat{w}_3$ & & & $5.502$ & $6.294$ \\
	$\hat{w}_4$ & & & $0.475$ & $-5.629$ \\
	$\hat{w}_5$ & & & $0.453$ & $4.425$ \\
	$\hat{w}_6$ & & & $0.435$ & $0.464$ \\
	$\hat{w}_7$ & & & & $0.493$ \\
	$\hat{w}_8$ & & & & $0.516$ \\
	$\hat{w}_9$ & & & & $0.533$ \\
\end{tabular}}
\end{subtable}
\begin{subtable}[!ht]{0.5\textwidth}
\centering
\scalebox{0.8}{
\begin{tabular}{l|rrrr}
	& $M = 0$ & $M = 1$ & $M = 6$ & $M = 9$ \\\hline
	$\hat{w}_0$ & $0.186$& $0.181$ & $0.133$ & $-0.077$ \\
	$\hat{w}_1$ & & $-0.791$ & $4.974$ & $6.080$ \\
	$\hat{w}_2$ & & & $-0.150$ & $-12.746$ \\
	$\hat{w}_3$ & & & $4.964$ & $6.398$ \\
	$\hat{w}_4$ & & & $-0.027$ & $-5.494$ \\
	$\hat{w}_5$ & & & $-0.020$ & $4.586$ \\
	$\hat{w}_6$ & & & $-0.013$ & $0.646$ \\
	$\hat{w}_7$ & & & & $0.692$ \\
	$\hat{w}_8$ & & & & $0.729$ \\
	$\hat{w}_9$ & & & & $0.758$ \\
\end{tabular}}
\end{subtable}
}
\caption{Table 1.1 from Bishop replicated using (a) analytical solutions, (b) gradient descent, and (c) the Scipy \textsc{BFGS} optimizer}
\label{tbl:bishop-1-1}
\end{figure}

\begin{table}[!ht]
\centering
\makebox[\textwidth][c]{
\begin{tabular}{c|cccc}
& $M=0$ & $M=1$ & $M=3$ \\\hline
SSE & $3.774$ & $2.237$ & $0.350$\\\hline
$\nabla$ SSE analytic & $2.2\e{-16}$ & $(1.3\e{-15}, -1.3\e{-15})$ & $(-3.3\e{-14}, -6.4\e{-15},$ \\
& & & $ -5.8\e{-15}, -1.7\e{-14})$ \\\hline
$\nabla$ SSE numeric & $0$ & $(2.7\e{-15}, -1.3\e{-15})$ & $(-3.4\e{-14}, -6.2\e{-15},$\\
& & & $-7.7\e{-15}, -1.7\e{-14})$ \\
\end{tabular}
}
\caption{Values of the SSE and the analytical and numerical gradients for the given values of $M$ calculated using the $w$ coefficients calculated from the analytical solution.}
\label{tbl:sse}
\end{table}

There are three ways to calculate the maximum likelihood weight vector to minimize the $L2$ norm - (a) analytically with matrices, (b) using gradient descent, and (c), using Scipy's built-in optimizer. Figure \ref{fig:2-1} shows the plots from Bishop Figure 1.4 reconstructed using the analytical (blue), gradient descent (green), and Scipy \textsc{BFGS}  (red) methods for different values of $M$. The coefficients from all three methods for the different values of $M$ in Bishop's Table 1.1 are given in Table \ref{tbl:bishop-1-1}. In general, the analytic computation matched Figure 1.4 in Bishop very well.

The analytic function to compute the gradient of the SSE was compared against the numeric central difference method in Table \ref{tbl:sse}, using the weights computed by the analytic method for the various values of $M$. The methods match, so the analytic function is a valid one to use for further optimization.

When recreating Bishop's Figure 1.4 using gradient descent and the \textsc{BFGS}, the same initial guess was given, and the analytic formula of the SSE gradient was used, for simplicity and consistency. Choosing initial guesses, step sizes, and thresholds for gradient descent strongly influences the results. In general, the closer the initial guess is to the analytic solution, the more likely the gradient descent function will find it. This is because the gradient of the SSE has multiple zeros - if there is an optimization for $9$ weights, then the SSE function is degree $9$, which means it can at most have $4$ local minima, according to Euler's theorem. This means that as in the discussion of Exercise 1 above, the gradient descent function will find different minima according to the start value. The threshold and step size were chosen to be $0.001$ and $0.01$ respectively, so that the gradient descent function came as close to the analytic solutions without taking too long to compute. The \textsc{BFGS} optimizer behaved like the gradient descent minimizer, except no threshold or step size needed to be given. However, it was strongly influenced by the initial guess, as discussed above.

\section{Exercise 3}

\begin{figure}[!ht]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.25\textwidth]{exercise3-1.pdf}}
	\caption{Predicted regressions using ridge regression for various values of $M$ and $\lambda$}
	\label{fig:3-1}
\end{figure}

Figure \ref{fig:3-1} shows the plots of predicted regression values using ridge regression for various values of $M$ and $\lambda$. Because a value of $\lambda=0$ corresponds to no regularizer, the plot for $M=9$ looks like the plots from Exercise 2 above. As $\lambda$ increases to $1$ and greater for all values of $M$, the predicted regression becomes more flat, since the regularizer punishes having large coefficients. Small values of $\lambda$ that are not $0$ yield good results. There is no benefit to increasing $M$ after a certain point when regressing with a regularizer.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{exercise3-2-1.pdf}
	\caption{The optimal regression lines for (a) Set A and (b) Set B and their corresponding $M$ and $\lambda$ values. (c) is a plot of all the data together (sets A and B as well as the training data)}
	\label{fig:3-2}
\end{figure}

There were two sets, A and B, in addition to a validation set given. To do model selection and test the results, I split each set A or B into two sets: half (arbitrarily the first half) became the training set, and half became the validation set (arbitrarily the second half). The validation set became the training set that the errors were reported on. To find the best regression, first choose an $M$ value using ridge regression with $\lambda = 0$ and iterating over all possible values of $M$ as there are data points. The $M$ that gives the smallest MSE for the training set, use that to test values of $\lambda$ that minimize the MSE on the validation set. The results of using set A as training vs. set B as training is given in Figure \ref{fig:3-2}. The SSE for set A on the test set is $0.91$ and the MSE for set B on the test set was $1.25$. The reason that set B gives a more complex model is that the data used in the training set was not uniformly distributed around the optimal regression line - the errors from the training set were pulling the model coefficients into higher complexity. In reality, neither model accurately describes all given data. This means that the choice of training data selection from the entire dataset strongly affects the regression output.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth]{exercise3-2-3.pdf}
	\caption{A plot of the MSE (mean square error) as a function of different values of $\lambda$}
	\label{fig:3-3}
\end{figure}

For the blogset data, a similar procedure was used. The training data was used to select the model class $M$. Values of $M \neq 2$ with $\lambda = 0$ yielded singular matrices when solving for optimal $w$, so the model class was chosen as $M = 1$ (MSE for $M = 1, \lambda = 0$ on the training set was $880.597$). Then the validation set was used to find the optimal value of $\lambda$. At the optimal value of $\lambda$ for $M = 1$ on the validation set, the MSE was $2641.527$. Figure \ref{fig:3-3} shows a plot of the MSE as a function of $\lambda$ for $M = 1$. With the optimal value of $\lambda$ chosen as the inflection point of the graph, the optimal parameters and test MSE were found to be $M = 1, \lambda = 0.00043,$ MSE = $2641.527$. 

\section{Exercise 4}

This is a test. 

\end{document}